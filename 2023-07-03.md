# Summary


| Date   | Notes
| :----- | :-------------------------------
| June 26 | Researched RL, gym, and autonomous cars
| June 27 | Researched gym environments
| June 28 | Researched MineRL further and followed tutorials
| June 29 | Debugged MineRL issues and followed documentation & articles
| June 30 | Researched and implemented REINFORCE in Cartpole


# Activities


Implemented CartPole REINFORCE: I successfully implemented the REINFORCE algorithm in the CartPole environment. This involved designing and training a neural network model to approximate the policy, defining a reward function, and implementing the backpropagation algorithm to update the model's weights. Through iterations and experimentation, I achieved a working implementation of the REINFORCE algorithm for the CartPole problem.


Working on implementing REINFORCE in MineRL: I have been working on extending the REINFORCE algorithm to the more complex MineRL environment. This involves additional steps compared to the CartPole implementation. Specifically, I have implemented a Convolutional Neural Network (CNN) architecture suitable for processing the POV RGB images from the MineRL environment.




List of most of the activities I’ve done: 

Researched Gym Environments and created doc on environments

Researched Cartpole Environment

Created Repo for Cartpole

Researched and read articles NNs, RL, ML, Policy Gradients, etc.

Learned numpy, Matplotlib, and PyTorch

Created Master Doc

Researched MineRL documentation

Debugging Issues

Researching issue solutions

Researched MineRL Environment Actions, States, Rewards, and Stages.


# Issues


I encountered several issues during the project, some of which I’ve listed below:


Difficulties in setting up the MineRL environment: Despite following the tutorial documentation, I faced compatibility issues, dependency errors, and problems configuring the basic environment. Specifically, I was unable to install the MineRl package and faced compatibility issues and import errors. These challenges hindered my progress and complicated the development process.


Challenges in implementing REINFORCE in CartPole: The implementation of the REINFORCE algorithm in the CartPole environment presented various challenges. Building and training the neural network model and optimizing the agent's performance required significant effort and troubleshooting.


# Plans


Reading more articles, working on REINFORCE in MineRL, and learning more!


# Article Summaries


Policy Gradient Methods for Reinforcement Learning with Function Approximation
Value-Function Approach, which has been predominant has several limitations:
- Orientated toward finding deterministic policies -- Optimal policy is typically stochastic
- Arbitrary small changes may or may not be selected


Policy Gradient Approach -- Policy Parameters are updated approximately proportional to the gradient
- Small changes are used


The paper proves an unbiased estimate of the gradient.


The policy Gradient Theorem addresses the problem of finding the gradient of the performance metric. Provides a foundation for updating policies in RL with function approximation.


Overall, this paper takes a step toward proving Policy Gradients as a valuable tool in RL.
