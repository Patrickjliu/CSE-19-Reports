# Summary


| Date   | Notes
| :----- | :-------------------------------
| July 31 | Worked on A2C
| August 1 | Worked on A2C
| August 2 | Implementing RNN in MineRL
| August 3 | RaceCar REINFORCE
| August 4 | RaceCar REINFORCE


# Activities


MineRL A2C: I successfully resolved the issues related to erratic camera movements by clamping camera values and reworking the sampling and generation of camera actions. I also implemented a 50-layer ResNet and reward shaping, which utilized vectors to reward the agent's movement within the environment. I also refined the A2C hyperparameters.

RaceCar REINFORCE: I successfully implemented REINFORCE in the RaceCar environment. This involved setting up the loss function, CNN, and managing multiple agents within the environment.


# Issues


MineRL: My agent has encountered challenges in obtaining logs effectively. I am currently exploring Imitation Learning as a potential solution to consistently collect logs and planks and other items.

RaceCar: I've observed instances where the agent becomes stationary after around 5-8 episodes. However, I haven't had the chance to conduct extensive training. Additionally, the training process for RaceCar is time-consuming.


# Plans


I plan on looking into Behavioral Cloning and Imitation Learning. Additionally, I plan on implementing weights and biases, and Stable Baselines within MineRL and RaceCar.


# Papers


Attention Is All You Need [https://arxiv.org/pdf/1706.03762.pdf]

This paper introduces the Transformer architecture, a novel approach to Natural Language Processing (NLP) that heavily relies on multi-headed attention mechanisms. The Transformer utilizes an encoder and decoder architecture, entirely built upon attention, which brings significant improvements in translation tasks such as English-German and English-French translations. Additionally, this architecture can be trained significantly faster than other models.