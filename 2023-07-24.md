# Summary


| Date   | Notes
| :----- | :-------------------------------
| June 17 | Worked on A2C
| June 28 | Worked on A2C
| June 19 | Worked on PPO
| June 20 | Worked on PPO
| June 21 | Worked on A2C


# Activities


Working on implementing A2C: I've built a functional actor-critic model, reward function, and saving and leading models. I've been focusing on refining my loss function, making my training more stable, and safely being able to quit training. I am also working on resolving the socket timeout issue likely related to a memory leak, implementing try-except statements to handle the environment's step and reset calls, and occasionally rebooting the environment.


Working on implementing PPO in MineRL: I've built a working actor-critic model, reward function, and saving and leading models. I've been focusing on working on refining my loss function.


Revising camera actions: I observed issues during training where my agent looked up while turning left or down while turning right. I'm working on adjusting the loss function and model to generate proper camera action outputs and balance the agent's actions and camera actions.


# Issues


Socket Timeout:  I have encountered an issue with my environment timing out that seems related to a memory leak. I've commented on a GitHub issue and am working on implementing try-except statements and rebooting the environment as needed to ensure stability. [https://github.com/minerllabs/minerl/issues/725]


Camera Actions: My agent exhibits undesired behavior of looking up while turning left or looking down while turning right. I am refining the loss function and model architecture to generate proper camera actions and achieve a better balance in the weight of the actions and camera actions.


# Plans


- Reading more articles on reinforcement learning to improve my implementation.
- Continue working on PPO and A2C in MineRL, refining loss function, and enhancing stability.
- Learning from past MineRL Diamond Competition Solutions


# Article Summaries


MineRL Diamond 2021 Competition: Overview, Results, and Lessons Learned [https://arxiv.org/pdf/2202.10583.pdf]


This Paper provides an overview of the 2021 Diamond Competition. It includes basic information about the competition and delves into the new changes within the competition structure to promote participation: Majorly, the inclusion of an intro track the promoted participation in MineRL.


Additionally, this paper reviews the results of the intro and research track. It highlights the top 5 solutions of each track and their solution to obtaining a diamond. Approaches include a sample-efficient hierarchical RL approach equipped with representation learning and imitation learning, utilizing k-means to cluster actions and then training a RNN with behavioral cloning, leveraging end-to-end RL, masked actions, and a new elaborate reward system.


Overall, this article reviews the improvements compared to the last competitions and plans for the future of the Minecraft Diamond Competition.